# Dagster instance configuration
# This file configures how Dagster stores run history, schedules, and other metadata.
#
# Where the dev/RPC instance stores local data:
# - If DAGSTER_HOME is set: instance root is $DAGSTER_HOME. Compute logs go to
#   $DAGSTER_HOME/storage/<run_id>/compute_logs; local artifact storage uses $DAGSTER_HOME.
# - If DAGSTER_HOME is not set: dagster dev uses the process cwd (this project dir) as the
#   instance base, so storage/ and compute logs end up under this repo.
# To put compute logs on a different disk, set DAGSTER_HOME to a path on that volume, or
# add a compute_logs section with LocalComputeLogManager and config.base_dir.
#
# If you see "No space left on device" when launching a run (SemLock in multiprocessing):
# on macOS this is usually the kernel semaphore limit, not disk. Reboot the Mac to clear
# semaphore state, or close other Python/apps that use multiprocessing.

# Use PostgreSQL for run storage (same DB as application data)
run_storage:
  module: dagster_postgres.run_storage
  class: PostgresRunStorage
  config:
    postgres_db:
      hostname:
        env: POSTGRES_HOST
      port:
        env: POSTGRES_PORT
      username:
        env: POSTGRES_USER
      password:
        env: POSTGRES_PASSWORD
      db_name:
        env: POSTGRES_DB

# Use PostgreSQL for event log storage
event_log_storage:
  module: dagster_postgres.event_log
  class: PostgresEventLogStorage
  config:
    postgres_db:
      hostname:
        env: POSTGRES_HOST
      port:
        env: POSTGRES_PORT
      username:
        env: POSTGRES_USER
      password:
        env: POSTGRES_PASSWORD
      db_name:
        env: POSTGRES_DB

# Use PostgreSQL for schedule storage
schedule_storage:
  module: dagster_postgres.schedule_storage
  class: PostgresScheduleStorage
  config:
    postgres_db:
      hostname:
        env: POSTGRES_HOST
      port:
        env: POSTGRES_PORT
      username:
        env: POSTGRES_USER
      password:
        env: POSTGRES_PASSWORD
      db_name:
        env: POSTGRES_DB

run_launcher:
  module: dagster.core.launcher
  class: DefaultRunLauncher

# Queue runs so we can cap concurrency (dagster 1.6.x format)
#   Connection budget (PostgreSQL max_connections=300):
#     Both app (talent_matching.db) and Dagster storage use NullPool — connections
#     are transient but overlap across many concurrent subprocesses.
#     Observed: ~6-8 transient connections per run (Dagster event log writes, run
#     status updates, app DB ops). 20 runs × 8 = 160 peak, plus ~30 overhead = 190.
#   Also bounded by 16 GB RAM: each run subprocess uses ~100-200 MB.
run_coordinator:
  module: dagster.core.run_coordinator
  class: QueuedRunCoordinator
  config:
    max_concurrent_runs: 20
    tag_concurrency_limits:
      - key: "dagster/concurrency_limit"
        value: "matchmaking"
        limit: 1

# Telemetry (disabled for privacy)
telemetry:
  enabled: false
